{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11893329,"sourceType":"datasetVersion","datasetId":7475683},{"sourceId":11893342,"sourceType":"datasetVersion","datasetId":7475695},{"sourceId":11904799,"sourceType":"datasetVersion","datasetId":7483616},{"sourceId":11893355,"sourceType":"datasetVersion","datasetId":7475704}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_name = \"google/flan-t5-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:21:46.929609Z","iopub.execute_input":"2025-05-23T02:21:46.929827Z","iopub.status.idle":"2025-05-23T02:22:50.445346Z","shell.execute_reply.started":"2025-05-23T02:21:46.929803Z","shell.execute_reply":"2025-05-23T02:22:50.444810Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"246e7e1541f04ad5bb5fc4b2ead217bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378e828e76a14a489b74d251298cb787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ae9b7b4e8f142f3bbeb0bbc7d58b277"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19faa33af793492db0ce5ecd7c92be7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c13b592136e42ef966260a5620a761d"}},"metadata":{}},{"name":"stderr","text":"2025-05-23 02:22:04.128424: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747966924.310605      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747966924.364154      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa40ca5e4d9a442899f132f4794a4815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ec9bf360e0444909c07ef31e511ee4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3624c6cc194547eb9c02973f37bd351f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe7743abfe164bf1b59f852b19d0356b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"725f39af9c664c16877fcf30a02a3846"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d827154d415e425c8282b2ddc586c2c3"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 2048)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 2048)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 2048)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"def clear_gpu():\n    import gc, torch\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\nclear_gpu()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:53:19.951321Z","iopub.execute_input":"2025-05-23T04:53:19.952195Z","iopub.status.idle":"2025-05-23T04:53:20.470795Z","shell.execute_reply.started":"2025-05-23T04:53:19.952160Z","shell.execute_reply":"2025-05-23T04:53:20.469712Z"}},"outputs":[],"execution_count":212},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:10.247881Z","iopub.execute_input":"2025-05-23T02:23:10.248573Z","iopub.status.idle":"2025-05-23T02:23:14.089238Z","shell.execute_reply.started":"2025-05-23T02:23:10.248549Z","shell.execute_reply":"2025-05-23T02:23:14.088633Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 2048)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 2048)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 2048)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n              (relative_attention_bias): Embedding(32, 32)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-23): 23 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=2048, out_features=2048, bias=False)\n              (k): Linear(in_features=2048, out_features=2048, bias=False)\n              (v): Linear(in_features=2048, out_features=2048, bias=False)\n              (o): Linear(in_features=2048, out_features=2048, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=2048, out_features=5120, bias=False)\n              (wi_1): Linear(in_features=2048, out_features=5120, bias=False)\n              (wo): Linear(in_features=5120, out_features=2048, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=2048, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"import json\nfile_path = \"/kaggle/input/classification/classification_instruction_data_en.jsonl\"\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = [json.loads(line.strip()) for line in f]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:37.685903Z","iopub.execute_input":"2025-05-23T02:23:37.686468Z","iopub.status.idle":"2025-05-23T02:23:37.712036Z","shell.execute_reply.started":"2025-05-23T02:23:37.686442Z","shell.execute_reply":"2025-05-23T02:23:37.711315Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"valid_labels = {\"negative\", \"positive\", \"neutral\"}\ndata = [sample for sample in data if sample['output'].strip().lower() in valid_labels]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:40.179497Z","iopub.execute_input":"2025-05-23T02:23:40.180202Z","iopub.status.idle":"2025-05-23T02:23:40.183774Z","shell.execute_reply.started":"2025-05-23T02:23:40.180175Z","shell.execute_reply":"2025-05-23T02:23:40.183209Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import re\n\ndef extract_answer(text):\n    matches = re.findall(r'= *\\$?(-?\\d+(?:\\.\\d+)?)', text)\n    if matches:\n        return matches[-1]  # lấy phép tính cuối cùng có dạng = xxx\n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:43.098340Z","iopub.execute_input":"2025-05-23T02:23:43.098604Z","iopub.status.idle":"2025-05-23T02:23:43.102852Z","shell.execute_reply.started":"2025-05-23T02:23:43.098584Z","shell.execute_reply":"2025-05-23T02:23:43.102097Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def extract_target(text):\n  for line in text.splitlines()[::-1]:\n    if line.strip().startswith(\"####\"):\n      return line.strip().replace(\"####\",\"\").strip()\n  return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:44.897219Z","iopub.execute_input":"2025-05-23T02:23:44.897765Z","iopub.status.idle":"2025-05-23T02:23:44.901915Z","shell.execute_reply.started":"2025-05-23T02:23:44.897744Z","shell.execute_reply":"2025-05-23T02:23:44.901253Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def split_cot_and_answer(output_text):\n    lines = output_text.strip().splitlines()\n    cot_lines = [line for line in lines if not line.strip().startswith(\"####\")]\n    answer = extract_target(output_text)\n    return \"\\n\".join(cot_lines), answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:47.710816Z","iopub.execute_input":"2025-05-23T02:23:47.711389Z","iopub.status.idle":"2025-05-23T02:23:47.715294Z","shell.execute_reply.started":"2025-05-23T02:23:47.711366Z","shell.execute_reply":"2025-05-23T02:23:47.714507Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: {data[i]['input'] } A: The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:23:50.191222Z","iopub.execute_input":"2025-05-23T02:23:50.191877Z","iopub.status.idle":"2025-05-23T02:24:25.585890Z","shell.execute_reply.started":"2025-05-23T02:23:50.191859Z","shell.execute_reply":"2025-05-23T02:24:25.585036Z"}},"outputs":[{"name":"stdout","text":"neutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\npositive\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nnegative\nnegative\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nThe answer is ?\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nnegative\npositive\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\n0.8\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Zero-shot + CoT (ReAct)","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}.\n  Q: {data[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint('Accuracy', acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:28:37.968784Z","iopub.execute_input":"2025-05-23T02:28:37.969161Z","iopub.status.idle":"2025-05-23T02:29:11.356195Z","shell.execute_reply.started":"2025-05-23T02:28:37.969139Z","shell.execute_reply":"2025-05-23T02:29:11.355405Z"}},"outputs":[{"name":"stdout","text":"neutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nnegative\nnegative\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nnegative\npositive\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nAccuracy 0.805\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: {data[i]['input'] } A: Let's think step by step. The sentiment is \"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:25:43.550383Z","iopub.execute_input":"2025-05-23T02:25:43.550630Z","iopub.status.idle":"2025-05-23T02:26:30.297186Z","shell.execute_reply.started":"2025-05-23T02:25:43.550614Z","shell.execute_reply":"2025-05-23T02:26:30.296556Z"}},"outputs":[{"name":"stdout","text":"0.8\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"code","source":"def zero_shot_ToT(text):\n    return (\n        f\"{text['instruction']}. Q: {text['input']}.\\n\"\n        f\"A: Let's think of three strategies for this question.\\n\"\n        f\"Strategy 1: \"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:54:48.879517Z","iopub.execute_input":"2025-05-23T02:54:48.880094Z","iopub.status.idle":"2025-05-23T02:54:48.883798Z","shell.execute_reply.started":"2025-05-23T02:54:48.880071Z","shell.execute_reply":"2025-05-23T02:54:48.883045Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"import re\n\ndef extract_labels_from_thoughts(text):\n    matches = re.findall(r'\\b(positive|negative|neutral)\\b', text.lower())\n    return list(set(matches))  # loại trùng nếu cần\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:43:09.785016Z","iopub.execute_input":"2025-05-23T02:43:09.785278Z","iopub.status.idle":"2025-05-23T02:43:09.789269Z","shell.execute_reply.started":"2025-05-23T02:43:09.785257Z","shell.execute_reply":"2025-05-23T02:43:09.788612Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"import re\n\ndef clean_label(text):\n    match = re.search(r\"\\b(positive|negative|neutral)\\b\", text.lower())\n    return match.group(1) if match else \"UNKNOWN\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:51:06.922537Z","iopub.execute_input":"2025-05-23T02:51:06.922783Z","iopub.status.idle":"2025-05-23T02:51:06.926510Z","shell.execute_reply.started":"2025-05-23T02:51:06.922767Z","shell.execute_reply":"2025-05-23T02:51:06.925862Z"}},"outputs":[],"execution_count":80},{"cell_type":"code","source":"def generate_strategy(prompt_base, strategy_index):\n    prompt = prompt_base + f\"Strategy {strategy_index + 1}:\\n\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n    output = model.generate(\n        **inputs,\n        max_length=100,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n        num_return_sequences=1\n    )\n    return tokenizer.decode(output[0], skip_special_tokens=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:23:07.857550Z","iopub.execute_input":"2025-05-23T04:23:07.857813Z","iopub.status.idle":"2025-05-23T04:23:07.862159Z","shell.execute_reply.started":"2025-05-23T04:23:07.857796Z","shell.execute_reply":"2025-05-23T04:23:07.861537Z"}},"outputs":[],"execution_count":198},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = zero_shot_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(3)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:42:36.597377Z","iopub.execute_input":"2025-05-23T04:42:36.597584Z","iopub.status.idle":"2025-05-23T04:44:31.711835Z","shell.execute_reply.started":"2025-05-23T04:42:36.597568Z","shell.execute_reply":"2025-05-23T04:44:31.711213Z"}},"outputs":[{"name":"stdout","text":"0.66\n","output_type":"stream"}],"execution_count":207},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = zero_shot_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(5)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:39:43.801834Z","iopub.execute_input":"2025-05-23T04:39:43.802555Z","iopub.status.idle":"2025-05-23T04:42:36.596250Z","shell.execute_reply.started":"2025-05-23T04:39:43.802529Z","shell.execute_reply":"2025-05-23T04:42:36.595516Z"}},"outputs":[{"name":"stdout","text":"0.6\n","output_type":"stream"}],"execution_count":206},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = zero_shot_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(10)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:44:31.712557Z","iopub.execute_input":"2025-05-23T04:44:31.712812Z","iopub.status.idle":"2025-05-23T04:49:42.049872Z","shell.execute_reply.started":"2025-05-23T04:44:31.712794Z","shell.execute_reply":"2025-05-23T04:49:42.049280Z"}},"outputs":[{"name":"stdout","text":"0.6\n","output_type":"stream"}],"execution_count":208},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"code","source":"from collections import Counter\ndef self_consistency(prompt, num_samples=5):\n    outputs = []\n    for _ in range(num_samples):\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        output = model.generate(\n            **inputs,\n            max_length=50,\n            do_sample=True,           \n            temperature=0.7,\n            top_k=50,\n            top_p=0.9,\n            num_return_sequences=1\n        )\n        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n        outputs.append(decoded)\n    return outputs\ndef majority_vote(outputs):\n    counts = Counter(outputs)\n    return counts.most_common(1)[0][0], counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:09:34.611219Z","iopub.execute_input":"2025-05-23T03:09:34.611889Z","iopub.status.idle":"2025-05-23T03:09:34.616542Z","shell.execute_reply.started":"2025-05-23T03:09:34.611869Z","shell.execute_reply":"2025-05-23T03:09:34.615940Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"def zero_shot_CoT(text):\n  return f\"\"\"\n  You are a classifier assistant.{text['instruction']}. \n  Q: {text['input'] } A: Let's think step by step. The sentiment is \"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:06:43.443035Z","iopub.execute_input":"2025-05-23T03:06:43.443298Z","iopub.status.idle":"2025-05-23T03:06:43.447000Z","shell.execute_reply.started":"2025-05-23T03:06:43.443280Z","shell.execute_reply":"2025-05-23T03:06:43.446410Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"prompt = zero_shot_CoT(data[0])\nprint(prompt)\nsamples = self_consistency(prompt, num_samples=5)\nbest_answer, all_votes = majority_vote(samples)\n\nprint(\"Self-Consistent Answer:\", best_answer)\nprint(\"All votes:\", all_votes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:09:36.906276Z","iopub.execute_input":"2025-05-23T03:09:36.906945Z","iopub.status.idle":"2025-05-23T03:09:37.504281Z","shell.execute_reply.started":"2025-05-23T03:09:36.906924Z","shell.execute_reply":"2025-05-23T03:09:37.503700Z"}},"outputs":[{"name":"stdout","text":"\n  You are a classifier assistant.Classify the following text into 'positive', 'neutral', or 'negative'.. \n  Q: between 1900 and 1920 where did most of the migrants to the united states come from A: Let's think step by step. The sentiment is \nSelf-Consistent Answer: neutral\nAll votes: Counter({'neutral': 5})\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt = zero_shot_CoT(data[i])\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    if best_answer == data[i]['output']:\n        acc = acc + 1\n    print(best_answer)\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:12:58.106061Z","iopub.execute_input":"2025-05-23T03:12:58.106615Z","iopub.status.idle":"2025-05-23T03:14:47.448331Z","shell.execute_reply.started":"2025-05-23T03:12:58.106594Z","shell.execute_reply":"2025-05-23T03:14:47.447703Z"}},"outputs":[{"name":"stdout","text":"neutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\n0.785\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt = zero_shot_CoT(data[i])\n    samples = self_consistency(prompt, num_samples=10)\n    best_answer, all_votes = majority_vote(samples)\n    if best_answer == data[i]['output']:\n        acc = acc + 1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:49:47.322524Z","iopub.execute_input":"2025-05-23T03:49:47.323084Z","iopub.status.idle":"2025-05-23T03:53:25.609547Z","shell.execute_reply.started":"2025-05-23T03:49:47.323061Z","shell.execute_reply":"2025-05-23T03:53:25.608866Z"}},"outputs":[{"name":"stdout","text":"0.78\n","output_type":"stream"}],"execution_count":142},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shot + CoT (ReAct)","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  few_shots_prompt = f\"\"\"\n  You're a sentiment classifier assistant. {data[i]['instruction']}, Recorrect each time you classifying with other LLM model\n  Example:Although I had high hopes, this product was a huge disappointment.\n  Let's think step by step.\n  1. The person had high hopes.\n  2. But the product was a disappointment.\n  3. The overall sentiment is negative.\n  Q: {data[i]['input'] } A: Let's think step by step. The sentiment is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:18:26.435731Z","iopub.execute_input":"2025-05-23T03:18:26.435932Z","iopub.status.idle":"2025-05-23T03:19:09.015504Z","shell.execute_reply.started":"2025-05-23T03:18:26.435917Z","shell.execute_reply":"2025-05-23T03:19:09.014888Z"}},"outputs":[{"name":"stdout","text":"0.815\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  few_shots_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: hi, A: positive\n  Q: Efficient and fast, A: positive\n  Q: How can I print mouse coordinates in Rust GTK on a drawing area when clicked?, A: neutral\n  Q: {data[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:19:09.016158Z","iopub.execute_input":"2025-05-23T03:19:09.016331Z","iopub.status.idle":"2025-05-23T03:19:50.919188Z","shell.execute_reply.started":"2025-05-23T03:19:09.016318Z","shell.execute_reply":"2025-05-23T03:19:50.918447Z"}},"outputs":[{"name":"stdout","text":"neutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nnegative\nneutral\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nnegative\nnegative\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\npositive\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nnegative\nnegative\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nnegative\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\npositive\nneutral\nneutral\nneutral\nneutral\nneutral\npositive\npositive\n0.805\n","output_type":"stream"}],"execution_count":104},{"cell_type":"markdown","source":"*thử đảo vị trí các ví dụ*","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(3, 200):\n  few_shots_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: How can I print mouse coordinates in Rust GTK on a drawing area when clicked?, A: neutral\n  Q: I hate this chat, A: negative\n  Q: Efficient and fast, A: positive\n  Q: {data[i]['input'] } A: The sentiment is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/197)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:22:35.533608Z","iopub.execute_input":"2025-05-23T03:22:35.534285Z","iopub.status.idle":"2025-05-23T03:23:15.368420Z","shell.execute_reply.started":"2025-05-23T03:22:35.534262Z","shell.execute_reply":"2025-05-23T03:23:15.367727Z"}},"outputs":[{"name":"stdout","text":"0.8071065989847716\n","output_type":"stream"}],"execution_count":107},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"code","source":"def few_shots_CoT_SC(query):\n    examples = find_top_k_tasks(query, 3)\n    prompt = build_prompt(examples, query)\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    \n    print(\"Self-Consistent Answer:\", best_answer)\n    print(\"All votes:\", all_votes)\nfor i in range(50):\n    few_shots_CoT_SC(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + Self-evaluation ToT ","metadata":{}},{"cell_type":"code","source":"def few_shots_ToT(text):\n    return f\"\"\"\n  You are a classifier assistant.{text['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: How can I print mouse coordinates in Rust GTK on a drawing area when clicked?, A: neutral\n  Q: I hate this chat, A: negative\n  Q: Efficient and fast, A: positive\n  Q: {text['input'] } A: The sentiment is ?\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:19:22.555350Z","iopub.execute_input":"2025-05-23T04:19:22.555901Z","iopub.status.idle":"2025-05-23T04:19:22.559510Z","shell.execute_reply.started":"2025-05-23T04:19:22.555879Z","shell.execute_reply":"2025-05-23T04:19:22.558898Z"}},"outputs":[],"execution_count":195},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = few_shots_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(3)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc + 1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:34:36.001802Z","iopub.execute_input":"2025-05-23T04:34:36.002146Z","iopub.status.idle":"2025-05-23T04:36:26.386090Z","shell.execute_reply.started":"2025-05-23T04:34:36.002126Z","shell.execute_reply":"2025-05-23T04:36:26.385327Z"}},"outputs":[{"name":"stdout","text":"0.82\n","output_type":"stream"}],"execution_count":203},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = few_shots_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(5)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc + 1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:26:27.560163Z","iopub.execute_input":"2025-05-23T04:26:27.560787Z","iopub.status.idle":"2025-05-23T04:29:10.088459Z","shell.execute_reply.started":"2025-05-23T04:26:27.560768Z","shell.execute_reply":"2025-05-23T04:29:10.087691Z"}},"outputs":[{"name":"stdout","text":"0.71\n","output_type":"stream"}],"execution_count":201},{"cell_type":"code","source":"acc = 0\nfor i in range(200):\n    prompt_base = few_shots_ToT(data[i])\n    strategies = [generate_strategy(prompt_base, j) for j in range(10)]\n    labels = extract_labels_from_thoughts(\"\\n\".join(strategies))\n    if labels:\n        prompt = followup_prompt(\", \".join(labels))\n        input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n        output = model.generate(\n            **input_ids,\n            max_length=20,\n            num_beams=5,\n            no_repeat_ngram_size=2,\n            early_stopping=True\n        )\n        final_label = tokenizer.decode(output[0], skip_special_tokens=True)\n        final_label = clean_label(final_label)\n        if final_label == data[i]['output']:\n            acc = acc + 1\nprint(acc/200)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:29:10.089708Z","iopub.execute_input":"2025-05-23T04:29:10.089922Z","iopub.status.idle":"2025-05-23T04:34:06.848865Z","shell.execute_reply.started":"2025-05-23T04:29:10.089905Z","shell.execute_reply":"2025-05-23T04:34:06.848132Z"}},"outputs":[{"name":"stdout","text":"0.635\n","output_type":"stream"}],"execution_count":202},{"cell_type":"markdown","source":"# QA","metadata":{}},{"cell_type":"markdown","source":"(hỏi đáp đơn giản)","metadata":{}},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:53:44.266364Z","iopub.execute_input":"2025-05-23T04:53:44.266628Z","iopub.status.idle":"2025-05-23T04:53:44.643318Z","shell.execute_reply.started":"2025-05-23T04:53:44.266608Z","shell.execute_reply":"2025-05-23T04:53:44.642543Z"}},"outputs":[],"execution_count":213},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"code","source":"qa = []\nwith open('/kaggle/input/simple-qa/simple_qa.json', 'r', encoding='utf-8') as f:\n    qa.append(json.load(f))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:56:07.354037Z","iopub.execute_input":"2025-05-23T04:56:07.354671Z","iopub.status.idle":"2025-05-23T04:56:07.381120Z","shell.execute_reply.started":"2025-05-23T04:56:07.354650Z","shell.execute_reply":"2025-05-23T04:56:07.380539Z"}},"outputs":[],"execution_count":216},{"cell_type":"code","source":"qa = qa[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:56:38.291191Z","iopub.execute_input":"2025-05-23T04:56:38.291445Z","iopub.status.idle":"2025-05-23T04:56:38.295059Z","shell.execute_reply.started":"2025-05-23T04:56:38.291429Z","shell.execute_reply":"2025-05-23T04:56:38.294266Z"}},"outputs":[],"execution_count":220},{"cell_type":"code","source":"qa[:3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:57:07.775406Z","iopub.execute_input":"2025-05-23T04:57:07.775975Z","iopub.status.idle":"2025-05-23T04:57:07.780449Z","shell.execute_reply.started":"2025-05-23T04:57:07.775954Z","shell.execute_reply":"2025-05-23T04:57:07.779852Z"}},"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"[{'instruction': 'Answer the question',\n  'input': 'between 1900 and 1920 where did most of the migrants to the united states come from',\n  'output': 'During the period between 1900 and 1920, most migrants to the United States came from Europe, particularly from Italy, Poland, and Russia. There were also significant numbers of immigrants from Canada, Mexico, and other parts of Latin America. In addition, there were smaller numbers of immigrants from Asia and other parts of the world.'},\n {'instruction': 'Answer the question',\n  'input': 'What does inioluwa mean',\n  'output': 'Inioluwa is a Nigerian name that means \"God\\'s gift\" in Yoruba, a language spoken in Nigeria. It is typically given to a child who is seen as a blessing from God.'},\n {'instruction': 'Answer the question',\n  'input': \"Today let's pretend we are building a 16-bit computer.\",\n  'output': \"Sure, I'd be happy to help you with building a 16-bit computer. What specific components or aspects of the computer would you like to focus on?\"}]"},"metadata":{}}],"execution_count":223},{"cell_type":"code","source":"def zero_shot_prompt(text):\n    return f\"Q: {text['input']}. A: Answer the question \"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T04:59:07.561660Z","iopub.execute_input":"2025-05-23T04:59:07.561933Z","iopub.status.idle":"2025-05-23T04:59:07.565751Z","shell.execute_reply.started":"2025-05-23T04:59:07.561915Z","shell.execute_reply":"2025-05-23T04:59:07.565011Z"}},"outputs":[],"execution_count":225},{"cell_type":"code","source":"print(qa[0]['input'])\nprint(qa[0]['output'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:01:52.632499Z","iopub.execute_input":"2025-05-23T05:01:52.633206Z","iopub.status.idle":"2025-05-23T05:01:52.637215Z","shell.execute_reply.started":"2025-05-23T05:01:52.633177Z","shell.execute_reply":"2025-05-23T05:01:52.636542Z"}},"outputs":[{"name":"stdout","text":"between 1900 and 1920 where did most of the migrants to the united states come from\nDuring the period between 1900 and 1920, most migrants to the United States came from Europe, particularly from Italy, Poland, and Russia. There were also significant numbers of immigrants from Canada, Mexico, and other parts of Latin America. In addition, there were smaller numbers of immigrants from Asia and other parts of the world.\n","output_type":"stream"}],"execution_count":229},{"cell_type":"code","source":"input = tokenizer(zero_shot_prompt(qa[0]), return_tensors='pt').to('cuda')\noutput = model.generate(\n    **input,\n    max_length = 100,\n    num_beams=5,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:01:00.966199Z","iopub.execute_input":"2025-05-23T05:01:00.966462Z","iopub.status.idle":"2025-05-23T05:01:01.084188Z","shell.execute_reply.started":"2025-05-23T05:01:00.966443Z","shell.execute_reply":"2025-05-23T05:01:01.083573Z"}},"outputs":[{"name":"stdout","text":"Mexico\n","output_type":"stream"}],"execution_count":227},{"cell_type":"code","source":"test_error = \"\"\"Q: In the early 20th century, where did most of the migrants to the united states come from\n. A: Answer the question, which's country ? \"\"\"\n\ninput = tokenizer(test_error, return_tensors='pt').to('cuda')\noutput = model.generate(\n    **input,\n    max_length = 100,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:10:42.501852Z","iopub.execute_input":"2025-05-23T05:10:42.502139Z","iopub.status.idle":"2025-05-23T05:10:42.689206Z","shell.execute_reply.started":"2025-05-23T05:10:42.502119Z","shell.execute_reply":"2025-05-23T05:10:42.688464Z"}},"outputs":[{"name":"stdout","text":"finland\n","output_type":"stream"}],"execution_count":247},{"cell_type":"markdown","source":"moi lan mot nuoc ~","metadata":{}},{"cell_type":"code","source":"print(qa[1]['input'])\nprint(qa[1]['output'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:12:08.276332Z","iopub.execute_input":"2025-05-23T05:12:08.277185Z","iopub.status.idle":"2025-05-23T05:12:08.281157Z","shell.execute_reply.started":"2025-05-23T05:12:08.277156Z","shell.execute_reply":"2025-05-23T05:12:08.280415Z"}},"outputs":[{"name":"stdout","text":"What does inioluwa mean\nInioluwa is a Nigerian name that means \"God's gift\" in Yoruba, a language spoken in Nigeria. It is typically given to a child who is seen as a blessing from God.\n","output_type":"stream"}],"execution_count":251},{"cell_type":"code","source":"input = tokenizer(zero_shot_prompt(qa[1]), return_tensors='pt').to('cuda')\noutput = model.generate(\n    **input,\n    max_length = 100,\n    do_sample=True,\n    temperature=0.7,\n    top_p=0.95,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:12:29.576695Z","iopub.execute_input":"2025-05-23T05:12:29.576952Z","iopub.status.idle":"2025-05-23T05:12:29.751808Z","shell.execute_reply.started":"2025-05-23T05:12:29.576932Z","shell.execute_reply":"2025-05-23T05:12:29.751199Z"}},"outputs":[{"name":"stdout","text":"(IV).\n","output_type":"stream"}],"execution_count":256},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"code","source":"def zero_shot_CoT(text):\n    return f\"Q: {text['input']}. A: Let's think step by step\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:13:16.804553Z","iopub.execute_input":"2025-05-23T05:13:16.804835Z","iopub.status.idle":"2025-05-23T05:13:16.808653Z","shell.execute_reply.started":"2025-05-23T05:13:16.804818Z","shell.execute_reply":"2025-05-23T05:13:16.808101Z"}},"outputs":[],"execution_count":257},{"cell_type":"code","source":"input = tokenizer(zero_shot_CoT(qa[0]), return_tensors='pt').to('cuda')\noutput = model.generate(\n    **input,\n    max_length = 100,\n    num_beams=5,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T05:13:45.855481Z","iopub.execute_input":"2025-05-23T05:13:45.855897Z","iopub.status.idle":"2025-05-23T05:13:47.264300Z","shell.execute_reply.started":"2025-05-23T05:13:45.855877Z","shell.execute_reply":"2025-05-23T05:13:47.263665Z"}},"outputs":[{"name":"stdout","text":"The majority of migrants to the United States between 1900 and 1920 came from Europe. So, the final answer is Europe.\n","output_type":"stream"}],"execution_count":259},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"markdown","source":"One-shot + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Few-shots + ToT","metadata":{}},{"cell_type":"markdown","source":"# Computation","metadata":{}},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"code","source":"prompt = f\"Q: {math[0]['input'] } A: Let's think step by step.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(**inputs, max_length=160)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"math[5]['output']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = f\"Q: {math[5]['input'] } A: Let's think step by step. The answer is ?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n    **inputs,\n    max_length=100,\n    num_beams=2,\n    no_repeat_ngram_size=3,\n    early_stopping=True\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(10):\n  zero_shot_prompt = f\"\"\"\n  You are a math assistant. When you need to calculate something, use the CALC() function.\n  Q: {math[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n\n  answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(results)):\n  print(results[i])\n  print(extract_target(math[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfew_shots_prompt = f\"\"\"\n Q: {math[0]['input']}. Let's think step by step',\n  \"\"\"\ninputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=2,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\nanswer =tokenizer.decode(outputs[0], skip_special_tokens=True)\nresults.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(answer)\nprint(extract_target(math[1]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(10):\n  zero_shot_prompt = f\"\"\"\n  You are a math assistant. When you need to calculate something, use the CALC() function.\n  Q: {math[i]['input'] } A: You need to answer follow this form: The answer: \"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(de)\n  answer = extract_answer(de)\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n  print(math[i]['input'])\n  print(math[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"code","source":"acc  = 0\nresults = []\nfor i in range(3,20):\n  few_shots_prompt = f\"\"\"\n  You are a math assistant. Whenever you see a math expression, call the function CALC().\nQ: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nA: Natalia sold CALC(48/2)=24 clips in May.\nNatalia sold CALC(48+24)=72 clips altogether in April and May.\n#### 72\nQ: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\nA: Weng earns $CALC(12/60)=0.2 per minute.\nWorking 50 minutes, she earned $CALC(0.2*50)=10,\n#### 10\nQ: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\nA: In the beginning, Betty has only $CALC(100/2) = 50.\nBetty's grandparents gave her $CALC(15*2)=30.\nThis means, Betty needs $CALC(100-50-30-15) = 5 more.\n#### 5\nQ: {math[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\n  answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfew_shots_prompt = f\"\"\"Q: Janet’s ducks lay 16 eggs per day.\n  She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n  She sells the remainder at the farmers' market daily for $2 per fresh duck egg.\n  How much in dollars does she make every day at the farmers' market?\n A: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n The answer: 18.\n\n Q: {math[1]['input']}. Let's think step by step',\n  \"\"\"\ninputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=2,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\nanswer =tokenizer.decode(outputs[0], skip_special_tokens=True)\nresults.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef split_steps(text):\n    text = text.replace('\\n', ' ')\n    math_expr = re.findall(r'[\\$\\d\\s\\+\\-\\*/xX=\\.]+', text)\n\n    steps = []\n    for expr in math_expr:\n        expr = expr.strip()\n        expr = expr.replace('x', '*').replace('X', '*')  # chuẩn hóa phép nhân\n        if '=' in expr and any(c.isdigit() for c in expr):\n            steps.append(expr)\n\n    return steps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nwith open(\"/kaggle/input/math-inference/final_train_math (1).json\") as f:\n    task_samples = [json.loads(line) for line in f]\nnew_tasks = []\nfor task in task_samples:\n    cot, answer = split_cot_and_answer(task[\"output\"])\n    task[\"chain_of_thought\"] = cot\n    task[\"answer\"] = answer\n    new_tasks.append(task)\n\nwith open(\"math_with_cot.jsonl\", \"w\") as f:\n    for task in new_tasks:\n        f.write(json.dumps(task) + \"\\n\")\ntexts = [task['input'] for task in new_tasks]\nvectorizer = TfidfVectorizer().fit(texts)\nX = vectorizer.transform(texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_similar_task(query):\n    q = vectorizer.transform([query])\n    idx = cosine_similarity(q, X).argmax()\n    return task_samples[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"one-shot + CoT + ART","metadata":{}},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n            \n            Now solve this task:\n            Input: {query}\n            Chain of Thought:\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_shot_ART(query):\n    example = find_similar_task(query)\n    prompt = build_prompt(example, query)\n    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n    outputs = model.generate(\n      **inputs,\n      max_length=150,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"one_shot_ART(\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_tasks= []\nwith open(\"/kaggle/input/math-inference/final_test_math (1).json\") as f:\n    test_tasks = [json.loads(line) for line in f]\nfor i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n            Now solve this task:\n            Input: {query}\n            Solution:\nLet's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n                        You're a math assistant\n            Now solve this task:\n            Input: {query}\n            Solution:\nLet's solve this step-by-step. Write your reasoning clearly. Recorrect itself. Final answer: ####\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + CoT + ART","metadata":{}},{"cell_type":"code","source":"def build_prompt(examples, query):\n    prompt = \"\"\n    for ex in examples:\n        cot = \"\\n\".join(ex.get(\"chain_of_thought\", [])) if \"chain_of_thought\" in ex else ex[\"output\"]\n        prompt += f\"\"\"Task: {ex.get('instruction', 'Math Problem')}\n                    Input: {ex['input']}\n                    Chain of Thought:\n                    {cot}\n                    Output: {ex['output']}\n                    \n                    \"\"\"\n\n    prompt += (\n        \"Now solve this task:\\n\"\n        f\"Input: {query}\\n\"\n        \"Solution:\\n\"\n        \"Let's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\n    )\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_tasks(query, k=3):\n    q = vectorizer.transform([query])\n    scores = cosine_similarity(q, X).flatten()\n    top_indices = scores.argsort()[::-1][:k]\n    return [task_samples[i] for i in top_indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_tasks(query, k=3):\n    q = vectorizer.transform([query])\n    scores = cosine_similarity(q, X).flatten()\n    top_indices = scores.argsort()[::-1][:k]\n    return [task_samples[i] for i in top_indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    few_shots_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    few_shots_ART(test_tasks[i]['input'], 5)\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Làm sao để giải quyết được OOM: prompt dài ??? -> prompt compression","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT(ReAct) + SELF-CONSITENCY (SC)","metadata":{}},{"cell_type":"code","source":"def self_consistency(prompt, num_samples=5):\n    outputs = []\n    for _ in range(num_samples):\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        output = model.generate(\n            **inputs,\n            max_length=150,\n            do_sample=True,           # sampling ON\n            temperature=0.7,\n            top_k=50,\n            top_p=0.9,\n            num_return_sequences=1\n        )\n        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n        print(decoded)\n        outputs.append(decoded)\n    return outputs\ndef majority_vote(outputs):\n    answers = [extract_answer(out) for out in outputs]\n    counts = Counter(answers)\n    return counts.most_common(1)[0][0], counts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nquery = test_tasks[0]['input']\nprompt = build_prompt(find_similar_task(query), query)\nprint(prompt)\nsamples = self_consistency(prompt, num_samples=5)\nbest_answer, all_votes = majority_vote(samples)\n\nprint(\"Self-Consistent Answer:\", best_answer)\nprint(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_shots_CoT_SC(query):\n    prompt = build_prompt(find_similar_task(query), query)\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    \n    print(\"Self-Consistent Answer:\", best_answer)\n    print(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shots_CoT_SC(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(examples, query):\n    prompt = \"\"\n    for ex in examples:\n        cot = \"\\n\".join(ex.get(\"chain_of_thought\", [])) if \"chain_of_thought\" in ex else ex[\"output\"]\n        prompt += f\"\"\"Task: {ex.get('instruction', 'Math Problem')}\n                    Input: {ex['input']}\n                    Chain of Thought:\n                    {cot}\n                    Output: {ex['output']}\n                    \n                    \"\"\"\n\n    prompt += (\n        \"Now solve this task:\\n\"\n        f\"Input: {query}\\n\"\n        \"Solution:\\n\"\n        \"Let's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\n    )\n    return prompt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def few_shots_CoT_SC(query):\n    examples = find_top_k_tasks(query, 3)\n    prompt = build_prompt(examples, query)\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    \n    print(\"Self-Consistent Answer:\", best_answer)\n    print(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(50):\n    few_shots_CoT_SC(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + ToT","metadata":{}},{"cell_type":"markdown","source":"# Reasoning","metadata":{}},{"cell_type":"code","source":"import json\nfile_path = \"/kaggle/working/reasoning_entries.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = [json.loads(line.strip()) for line in f]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"code","source":"def zero_shot_CoT(text):\n    return f\"Q: {text['input']}. A: Let's think step by step and explain\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(zero_shot_CoT(data[0]), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n    **input, \n    max_length=100,\n    num_beams=5,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = []\ntarget = []\nfor i in range(len(val_data)):\n    input = tokenizer(zero_shot_CoT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    answer.append(tokenizer.decode(output[0], skip_special_token=True))\n    target.append(val_data[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(val_data)):\n    print(answer[i])\n    print(target[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zero_shot_ToT(text):\n    return f\"Q: {text['input']}. A:Let's think of three different strategies for this question. For each strategy, explain the main idea and how it could be implemented.\"\ndef followup_prompt(text):\n return f\"Here are 3 possible situations: {text}. A: Which scenario is most likely to happen and why\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(zero_shot_ToT(data[0]), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\nanswer =tokenizer.decode(output[0], skip_special_token=True)\nprint(answer)\nfollowup_prompt(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(followup_prompt(answer), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\nprint(tokenizer.decode(output[0], skip_special_token=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"code","source":"answer_1 = []\nbranchs_1 = []\nfor i in range(len(val_data)):\n    input = tokenizer(zero_shot_ToT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    branch = tokenizer.decode(output[0], skip_special_token=True)\n    branchs_1.append(branch)\n    input = tokenizer(followup_prompt(branch), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n            **input, \n            max_length=100,\n            num_beams=5,\n            no_repeat_ngram_size=3,\n            early_stopping=False\n        )\n    answer = tokenizer.decode(output[0], skip_special_token=True)\n    print(answer)\n    answer_1.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"code","source":"def few_shots_CoT(text):\n    return f\"\"\"\nQ: Explain quantum computing in simple terms,\nA: Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In contrast to classical computers, which use bits to store and process information, quantum computers use quantum bits, or qubits. Because qubits can be in multiple states at once, quantum computers are able to perform many calculations simultaneously, which can make them much faster than classical computers for certain types of problems. While quantum computers are still in the early stages of development, they have the potential to revolutionize many fields, including cryptography, medicine, and materials science.\n\nQ: why shouldn't I trust birds?,\nA: There is no reason not to trust birds in general. However, wild birds may carry diseases and it's best to avoid handling them or coming into contact with their droppings. Also, some bird species, such as pigeons, can carry diseases such as histoplasmosis and cryptococcosis. So it's better to take precautions when interacting with wild birds.\n\nQ: I have an array of DomElements in JS, how can I find the item with the largest height?\nA: You can loop through the array of DomElements, and for each element, use the `offsetHeight` property to get its height. You can then compare this height to the current maximum height found so far and update the maximum height and corresponding DomElement accordingly. Here's an example implementation:\n```javascript\nfunction findMaxHeightElement(elements) {{\n  let maxHeight = -Infinity;\n  let maxHeightElement = null;\n\n  for (let i = 0; i < elements.length; i++) {{\n    const elementHeight = elements[i].offsetHeight;\n    if (elementHeight > maxHeight) {{\n      maxHeight = elementHeight;\n      maxHeightElement = elements[i];\n    }}\n  }}\n\n  return maxHeightElement;\n}}\nQ: {text['input']}\nA: Let's think step by step\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer2 = []\ntarget2 = []\nfor i in range(100):\n    input = tokenizer(few_shots_CoT(val_data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    answer2.append(tokenizer.decode(output[0], skip_special_token=True))\n    target2.append(val_data[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(100):\n    print(answer2[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Few-shots ToT","metadata":{}},{"cell_type":"code","source":"def few_shots_ToT(text):\n    return f\"\"\"\nQ: Explain quantum computing in simple terms,\nA: Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In contrast to classical computers, which use bits to store and process information, quantum computers use quantum bits, or qubits. Because qubits can be in multiple states at once, quantum computers are able to perform many calculations simultaneously, which can make them much faster than classical computers for certain types of problems. While quantum computers are still in the early stages of development, they have the potential to revolutionize many fields, including cryptography, medicine, and materials science.\n\nQ: why shouldn't I trust birds?,\nA: There is no reason not to trust birds in general. However, wild birds may carry diseases and it's best to avoid handling them or coming into contact with their droppings. Also, some bird species, such as pigeons, can carry diseases such as histoplasmosis and cryptococcosis. So it's better to take precautions when interacting with wild birds.\n\nQ: I have an array of DomElements in JS, how can I find the item with the largest height?\nA: You can loop through the array of DomElements, and for each element, use the `offsetHeight` property to get its height. You can then compare this height to the current maximum height found so far and update the maximum height and corresponding DomElement accordingly. Here's an example implementation:\n```javascript\nfunction findMaxHeightElement(elements) {{\n  let maxHeight = -Infinity;\n  let maxHeightElement = null;\n\n  for (let i = 0; i < elements.length; i++) {{\n    const elementHeight = elements[i].offsetHeight;\n    if (elementHeight > maxHeight) {{\n      maxHeight = elementHeight;\n      maxHeightElement = elements[i];\n    }}\n  }}\n\n  return maxHeightElement;\n}}\nQ: {text['input']}\nA:  A:Let's think of three different strategies for this question. For each strategy, explain the main idea and how it could be implemented.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer_2 = []\nbranchs_2 = []\nfor i in range(len(val_data)):\n    input = tokenizer(few_shots_ToT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    branch = tokenizer.decode(output[0], skip_special_token=True)\n    branchs_2.append(branch)\n    input = tokenizer(followup_prompt(branch), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n            **input, \n            max_length=100,\n            num_beams=5,\n            no_repeat_ngram_size=3,\n            early_stopping=False\n        )\n    answer = tokenizer.decode(output[0], skip_special_token=True)\n    print(answer)\n    answer_2.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetune","metadata":{}},{"cell_type":"code","source":"import json\nclassification = []\nwith open('classification_instruction_data_en.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        classification.append(json.loads(line.strip()))\nqa = []\ncomputation = []\nreasoning = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\nimport os\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel2 = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    save_steps=500,\n    logging_steps=100,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False\n)\ntrainer = Trainer(\n    model=model2,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model2)\n)\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}