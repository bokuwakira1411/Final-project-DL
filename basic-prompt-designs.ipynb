{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def clear_gpu():\n    import gc, torch\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.ipc_collect()\n\nclear_gpu()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\n\nmodel_name = \"google/flan-t5-xl\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification","metadata":{}},{"cell_type":"code","source":"import json\nfile_path = \"/content/classification_instruction_data_en.jsonl\"\n\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = [json.loads(line.strip()) for line in f]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T02:15:37.797026Z","iopub.execute_input":"2025-05-23T02:15:37.797364Z","iopub.status.idle":"2025-05-23T02:15:37.801246Z","shell.execute_reply.started":"2025-05-23T02:15:37.797303Z","shell.execute_reply":"2025-05-23T02:15:37.800303Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"valid_labels = {\"negative\", \"positive\", \"neutral\"}\ndata = [sample for sample in data if sample['output'].strip().lower() in valid_labels]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef extract_answer(text):\n    matches = re.findall(r'= *\\$?(-?\\d+(?:\\.\\d+)?)', text)\n    if matches:\n        return matches[-1]  # lấy phép tính cuối cùng có dạng = xxx\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def extract_target(text):\n  for line in text.splitlines()[::-1]:\n    if line.strip().startswith(\"####\"):\n      return line.strip().replace(\"####\",\"\").strip()\n  return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_cot_and_answer(output_text):\n    lines = output_text.strip().splitlines()\n    cot_lines = [line for line in lines if not line.strip().startswith(\"####\")]\n    answer = extract_target(output_text)\n    return \"\\n\".join(cot_lines), answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: {data[i]['input'] } A: The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT (ReAct)","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}.\n  Q: {data[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint('Accuracy', acc/300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: {data[i]['input'] } A: Let's think step by step. The sentiment is \"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shot + CoT (ReAct)","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  few_shots_prompt = f\"\"\"\n  You're a sentiment classifier assistant. {data[i]['instruction']}, Recorrect each time you classifying with other LLM model\n  Example:Although I had high hopes, this product was a huge disappointment.\n  Let's think step by step.\n  1. The person had high hopes.\n  2. But the product was a disappointment.\n  3. The overall sentiment is negative.\n  Q: {data[i]['input'] } A: Let's think step by step. The sentiment is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(200):\n  few_shots_prompt = f\"\"\"\n  You are a classifier assistant.{data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: hi, A: positive\n  Q: Efficient and fast, A: positive\n  Q: How can I print mouse coordinates in Rust GTK on a drawing area when clicked?, A: neutral\n  Q: {data[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(data[i]['output'])\n  print(de)\n  if de == data[i]['output']:\n    acc = acc+1\nprint(acc/200)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"*thử đảo vị trí các ví dụ*","metadata":{}},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(3, 100):\n  zero_shot_prompt = f\"\"\"\n  You are a classifier assistant.{train_data[i]['instruction']}. Recorrect each time you classifying with other LLM model\n  Q: How can I print mouse coordinates in Rust GTK on a drawing area when clicked?, A: neutral\n  Q: I hate this chat, A: negative\n  Q: Efficient and fast, A: positive\n  Q: {train_data[i]['input'] } A: The sentiment is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(train_data[i]['output'])\n  print(de)\n  if de == train_data[i]['output']:\n    acc = acc+1\nprint(acc/97)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One-shot + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Few-shots + ToT","metadata":{}},{"cell_type":"markdown","source":"# QA","metadata":{}},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"markdown","source":"One-shot + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + ART","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Few-shots + ToT","metadata":{}},{"cell_type":"markdown","source":"# Computation","metadata":{}},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"code","source":"prompt = f\"Q: {math[0]['input'] } A: Let's think step by step.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(**inputs, max_length=160)\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"math[5]['output']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = f\"Q: {math[5]['input'] } A: Let's think step by step. The answer is ?\"\ninputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n    **inputs,\n    max_length=100,\n    num_beams=2,\n    no_repeat_ngram_size=3,\n    early_stopping=True\n)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\nprint(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(10):\n  zero_shot_prompt = f\"\"\"\n  You are a math assistant. When you need to calculate something, use the CALC() function.\n  Q: {math[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n\n  answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(results)):\n  print(results[i])\n  print(extract_target(math[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfew_shots_prompt = f\"\"\"\n Q: {math[0]['input']}. Let's think step by step',\n  \"\"\"\ninputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=2,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\nanswer =tokenizer.decode(outputs[0], skip_special_tokens=True)\nresults.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(answer)\nprint(extract_target(math[1]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfor i in range(10):\n  zero_shot_prompt = f\"\"\"\n  You are a math assistant. When you need to calculate something, use the CALC() function.\n  Q: {math[i]['input'] } A: You need to answer follow this form: The answer: \"\"\"\n  inputs = tokenizer(zero_shot_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n  de = tokenizer.decode(outputs[0], skip_special_tokens=True)\n  print(de)\n  answer = extract_answer(de)\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n  print(math[i]['input'])\n  print(math[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"code","source":"acc  = 0\nresults = []\nfor i in range(3,20):\n  few_shots_prompt = f\"\"\"\n  You are a math assistant. Whenever you see a math expression, call the function CALC().\nQ: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\nA: Natalia sold CALC(48/2)=24 clips in May.\nNatalia sold CALC(48+24)=72 clips altogether in April and May.\n#### 72\nQ: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\nA: Weng earns $CALC(12/60)=0.2 per minute.\nWorking 50 minutes, she earned $CALC(0.2*50)=10,\n#### 10\nQ: Betty is saving money for a new wallet which costs $100. Betty has only half of the money she needs. Her parents decided to give her $15 for that purpose, and her grandparents twice as much as her parents. How much more money does Betty need to buy the wallet?\nA: In the beginning, Betty has only $CALC(100/2) = 50.\nBetty's grandparents gave her $CALC(15*2)=30.\nThis means, Betty needs $CALC(100-50-30-15) = 5 more.\n#### 5\nQ: {math[i]['input'] } A: Let's think step by step. The answer is ?\"\"\"\n  inputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\n  outputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\n  answer = extract_answer(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n  results.append(answer)\n  target = extract_target(math[i]['output'])\n  if answer == target:\n    acc = acc+1\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"acc = 0\nresults = []\nfew_shots_prompt = f\"\"\"Q: Janet’s ducks lay 16 eggs per day.\n  She eats three for breakfast every morning and bakes muffins for her friends every day with four.\n  She sells the remainder at the farmers' market daily for $2 per fresh duck egg.\n  How much in dollars does she make every day at the farmers' market?\n A: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n The answer: 18.\n\n Q: {math[1]['input']}. Let's think step by step',\n  \"\"\"\ninputs = tokenizer(few_shots_prompt, return_tensors=\"pt\").to('cuda')\noutputs = model.generate(\n      **inputs,\n      max_length=100,\n      num_beams=2,\n      no_repeat_ngram_size=3,\n      early_stopping=True\n  )\n\nanswer =tokenizer.decode(outputs[0], skip_special_tokens=True)\nresults.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef split_steps(text):\n    text = text.replace('\\n', ' ')\n    math_expr = re.findall(r'[\\$\\d\\s\\+\\-\\*/xX=\\.]+', text)\n\n    steps = []\n    for expr in math_expr:\n        expr = expr.strip()\n        expr = expr.replace('x', '*').replace('X', '*')  # chuẩn hóa phép nhân\n        if '=' in expr and any(c.isdigit() for c in expr):\n            steps.append(expr)\n\n    return steps","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nwith open(\"/kaggle/input/math-inference/final_train_math (1).json\") as f:\n    task_samples = [json.loads(line) for line in f]\nnew_tasks = []\nfor task in task_samples:\n    cot, answer = split_cot_and_answer(task[\"output\"])\n    task[\"chain_of_thought\"] = cot\n    task[\"answer\"] = answer\n    new_tasks.append(task)\n\nwith open(\"math_with_cot.jsonl\", \"w\") as f:\n    for task in new_tasks:\n        f.write(json.dumps(task) + \"\\n\")\ntexts = [task['input'] for task in new_tasks]\nvectorizer = TfidfVectorizer().fit(texts)\nX = vectorizer.transform(texts)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_similar_task(query):\n    q = vectorizer.transform([query])\n    idx = cosine_similarity(q, X).argmax()\n    return task_samples[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"one-shot + CoT + ART","metadata":{}},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n            \n            Now solve this task:\n            Input: {query}\n            Chain of Thought:\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_shot_ART(query):\n    example = find_similar_task(query)\n    prompt = build_prompt(example, query)\n    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n    outputs = model.generate(\n      **inputs,\n      max_length=150,\n      num_beams=5,\n      no_repeat_ngram_size=3,\n      early_stopping=False\n  )\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"one_shot_ART(\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_tasks= []\nwith open(\"/kaggle/input/math-inference/final_test_math (1).json\") as f:\n    test_tasks = [json.loads(line) for line in f]\nfor i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n            Now solve this task:\n            Input: {query}\n            Solution:\nLet's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(example, query):\n    cot = \"\\n\".join(example[\"chain_of_thought\"])\n    return f\"\"\"Here is a similar task:\n            Task: {example['instruction']}\n            Input: {example['input']}\n            Chain of Thought:\n            {cot}\n            Output: {example['output']}\n                        You're a math assistant\n            Now solve this task:\n            Input: {query}\n            Solution:\nLet's solve this step-by-step. Write your reasoning clearly. Recorrect itself. Final answer: ####\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shot_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + CoT + ART","metadata":{}},{"cell_type":"code","source":"def build_prompt(examples, query):\n    prompt = \"\"\n    for ex in examples:\n        cot = \"\\n\".join(ex.get(\"chain_of_thought\", [])) if \"chain_of_thought\" in ex else ex[\"output\"]\n        prompt += f\"\"\"Task: {ex.get('instruction', 'Math Problem')}\n                    Input: {ex['input']}\n                    Chain of Thought:\n                    {cot}\n                    Output: {ex['output']}\n                    \n                    \"\"\"\n\n    prompt += (\n        \"Now solve this task:\\n\"\n        f\"Input: {query}\\n\"\n        \"Solution:\\n\"\n        \"Let's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\n    )\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_tasks(query, k=3):\n    q = vectorizer.transform([query])\n    scores = cosine_similarity(q, X).flatten()\n    top_indices = scores.argsort()[::-1][:k]\n    return [task_samples[i] for i in top_indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_tasks(query, k=3):\n    q = vectorizer.transform([query])\n    scores = cosine_similarity(q, X).flatten()\n    top_indices = scores.argsort()[::-1][:k]\n    return [task_samples[i] for i in top_indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    few_shots_ART(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    few_shots_ART(test_tasks[i]['input'], 5)\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Làm sao để giải quyết được OOM: prompt dài ??? -> prompt compression","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT(ReAct) + SELF-CONSITENCY (SC)","metadata":{}},{"cell_type":"code","source":"def self_consistency(prompt, num_samples=5):\n    outputs = []\n    for _ in range(num_samples):\n        inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n        output = model.generate(\n            **inputs,\n            max_length=150,\n            do_sample=True,           # sampling ON\n            temperature=0.7,\n            top_k=50,\n            top_p=0.9,\n            num_return_sequences=1\n        )\n        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n        print(decoded)\n        outputs.append(decoded)\n    return outputs\ndef majority_vote(outputs):\n    answers = [extract_answer(out) for out in outputs]\n    counts = Counter(answers)\n    return counts.most_common(1)[0][0], counts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nquery = test_tasks[0]['input']\nprompt = build_prompt(find_similar_task(query), query)\nprint(prompt)\nsamples = self_consistency(prompt, num_samples=5)\nbest_answer, all_votes = majority_vote(samples)\n\nprint(\"Self-Consistent Answer:\", best_answer)\nprint(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def one_shots_CoT_SC(query):\n    prompt = build_prompt(find_similar_task(query), query)\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    \n    print(\"Self-Consistent Answer:\", best_answer)\n    print(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(10):\n    one_shots_CoT_SC(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_prompt(examples, query):\n    prompt = \"\"\n    for ex in examples:\n        cot = \"\\n\".join(ex.get(\"chain_of_thought\", [])) if \"chain_of_thought\" in ex else ex[\"output\"]\n        prompt += f\"\"\"Task: {ex.get('instruction', 'Math Problem')}\n                    Input: {ex['input']}\n                    Chain of Thought:\n                    {cot}\n                    Output: {ex['output']}\n                    \n                    \"\"\"\n\n    prompt += (\n        \"Now solve this task:\\n\"\n        f\"Input: {query}\\n\"\n        \"Solution:\\n\"\n        \"Let's solve this step-by-step. Write your reasoning clearly. Final answer: ####\"\n    )\n    return prompt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def few_shots_CoT_SC(query):\n    examples = find_top_k_tasks(query, 3)\n    prompt = build_prompt(examples, query)\n    samples = self_consistency(prompt, num_samples=5)\n    best_answer, all_votes = majority_vote(samples)\n    \n    print(\"Self-Consistent Answer:\", best_answer)\n    print(\"All votes:\", all_votes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(50):\n    few_shots_CoT_SC(test_tasks[i]['input'])\n    print(extract_target(test_tasks[i]['output']))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + ToT","metadata":{}},{"cell_type":"markdown","source":"# Reasoning","metadata":{}},{"cell_type":"code","source":"import json\nfile_path = \"/kaggle/working/reasoning_entries.json\"\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\n    data = [json.loads(line.strip()) for line in f]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Zero-shot**","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + CoT","metadata":{}},{"cell_type":"code","source":"def zero_shot_CoT(text):\n    return f\"Q: {text['input']}. A: Let's think step by step and explain\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(zero_shot_CoT(data[0]), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n    **input, \n    max_length=100,\n    num_beams=5,\n    no_repeat_ngram_size=3,\n    early_stopping=False\n)\nprint(tokenizer.decode(output[0], skip_special_tokens=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clear_gpu()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer = []\ntarget = []\nfor i in range(len(val_data)):\n    input = tokenizer(zero_shot_CoT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    answer.append(tokenizer.decode(output[0], skip_special_token=True))\n    target.append(val_data[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(len(val_data)):\n    print(answer[i])\n    print(target[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def zero_shot_ToT(text):\n    return f\"Q: {text['input']}. A:Let's think of three different strategies for this question. For each strategy, explain the main idea and how it could be implemented.\"\ndef followup_prompt(text):\n return f\"Here are 3 possible situations: {text}. A: Which scenario is most likely to happen and why\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(zero_shot_ToT(data[0]), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\nanswer =tokenizer.decode(output[0], skip_special_token=True)\nprint(answer)\nfollowup_prompt(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = tokenizer(followup_prompt(answer), return_tensors=\"pt\").to('cuda')\noutput = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\nprint(tokenizer.decode(output[0], skip_special_token=True))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zero-shot + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Zero-shot + ToT","metadata":{}},{"cell_type":"code","source":"answer_1 = []\nbranchs_1 = []\nfor i in range(len(val_data)):\n    input = tokenizer(zero_shot_ToT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    branch = tokenizer.decode(output[0], skip_special_token=True)\n    branchs_1.append(branch)\n    input = tokenizer(followup_prompt(branch), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n            **input, \n            max_length=100,\n            num_beams=5,\n            no_repeat_ngram_size=3,\n            early_stopping=False\n        )\n    answer = tokenizer.decode(output[0], skip_special_token=True)\n    print(answer)\n    answer_1.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Few-shots**","metadata":{}},{"cell_type":"markdown","source":"Few-shots + CoT","metadata":{}},{"cell_type":"code","source":"def few_shots_CoT(text):\n    return f\"\"\"\nQ: Explain quantum computing in simple terms,\nA: Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In contrast to classical computers, which use bits to store and process information, quantum computers use quantum bits, or qubits. Because qubits can be in multiple states at once, quantum computers are able to perform many calculations simultaneously, which can make them much faster than classical computers for certain types of problems. While quantum computers are still in the early stages of development, they have the potential to revolutionize many fields, including cryptography, medicine, and materials science.\n\nQ: why shouldn't I trust birds?,\nA: There is no reason not to trust birds in general. However, wild birds may carry diseases and it's best to avoid handling them or coming into contact with their droppings. Also, some bird species, such as pigeons, can carry diseases such as histoplasmosis and cryptococcosis. So it's better to take precautions when interacting with wild birds.\n\nQ: I have an array of DomElements in JS, how can I find the item with the largest height?\nA: You can loop through the array of DomElements, and for each element, use the `offsetHeight` property to get its height. You can then compare this height to the current maximum height found so far and update the maximum height and corresponding DomElement accordingly. Here's an example implementation:\n```javascript\nfunction findMaxHeightElement(elements) {{\n  let maxHeight = -Infinity;\n  let maxHeightElement = null;\n\n  for (let i = 0; i < elements.length; i++) {{\n    const elementHeight = elements[i].offsetHeight;\n    if (elementHeight > maxHeight) {{\n      maxHeight = elementHeight;\n      maxHeightElement = elements[i];\n    }}\n  }}\n\n  return maxHeightElement;\n}}\nQ: {text['input']}\nA: Let's think step by step\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer2 = []\ntarget2 = []\nfor i in range(100):\n    input = tokenizer(few_shots_CoT(val_data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    answer2.append(tokenizer.decode(output[0], skip_special_token=True))\n    target2.append(val_data[i]['output'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(100):\n    print(answer2[i])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Few-shots + CoT + SC","metadata":{}},{"cell_type":"markdown","source":"Few-shots ToT","metadata":{}},{"cell_type":"code","source":"def few_shots_ToT(text):\n    return f\"\"\"\nQ: Explain quantum computing in simple terms,\nA: Quantum computing is a type of computing that uses quantum-mechanical phenomena, such as superposition and entanglement, to perform operations on data. In contrast to classical computers, which use bits to store and process information, quantum computers use quantum bits, or qubits. Because qubits can be in multiple states at once, quantum computers are able to perform many calculations simultaneously, which can make them much faster than classical computers for certain types of problems. While quantum computers are still in the early stages of development, they have the potential to revolutionize many fields, including cryptography, medicine, and materials science.\n\nQ: why shouldn't I trust birds?,\nA: There is no reason not to trust birds in general. However, wild birds may carry diseases and it's best to avoid handling them or coming into contact with their droppings. Also, some bird species, such as pigeons, can carry diseases such as histoplasmosis and cryptococcosis. So it's better to take precautions when interacting with wild birds.\n\nQ: I have an array of DomElements in JS, how can I find the item with the largest height?\nA: You can loop through the array of DomElements, and for each element, use the `offsetHeight` property to get its height. You can then compare this height to the current maximum height found so far and update the maximum height and corresponding DomElement accordingly. Here's an example implementation:\n```javascript\nfunction findMaxHeightElement(elements) {{\n  let maxHeight = -Infinity;\n  let maxHeightElement = null;\n\n  for (let i = 0; i < elements.length; i++) {{\n    const elementHeight = elements[i].offsetHeight;\n    if (elementHeight > maxHeight) {{\n      maxHeight = elementHeight;\n      maxHeightElement = elements[i];\n    }}\n  }}\n\n  return maxHeightElement;\n}}\nQ: {text['input']}\nA:  A:Let's think of three different strategies for this question. For each strategy, explain the main idea and how it could be implemented.\n\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"answer_2 = []\nbranchs_2 = []\nfor i in range(len(val_data)):\n    input = tokenizer(few_shots_ToT(data[i]), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n        **input, \n        max_length=100,\n        num_beams=5,\n        no_repeat_ngram_size=3,\n        early_stopping=False\n    )\n    branch = tokenizer.decode(output[0], skip_special_token=True)\n    branchs_2.append(branch)\n    input = tokenizer(followup_prompt(branch), return_tensors=\"pt\").to('cuda')\n    output = model.generate(\n            **input, \n            max_length=100,\n            num_beams=5,\n            no_repeat_ngram_size=3,\n            early_stopping=False\n        )\n    answer = tokenizer.decode(output[0], skip_special_token=True)\n    print(answer)\n    answer_2.append(answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Finetune","metadata":{}},{"cell_type":"code","source":"import json\nclassification = []\nwith open('classification_instruction_data_en.jsonl', 'r', encoding='utf-8') as f:\n    for line in f:\n        classification.append(json.loads(line.strip()))\nqa = []\ncomputation = []\nreasoning = []","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq\nimport os\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments, DataCollatorForSeq2Seq\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel2 = T5ForConditionalGeneration.from_pretrained(model_name)\n\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    save_steps=500,\n    logging_steps=100,\n    save_total_limit=2,\n    remove_unused_columns=False,\n    push_to_hub=False\n)\ntrainer = Trainer(\n    model=model2,\n    args=training_args,\n    train_dataset=train_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer, model2)\n)\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}