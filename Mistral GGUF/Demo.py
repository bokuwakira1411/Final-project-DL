import streamlit as st
import gc
import os
import torch
import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from sklearn.feature_extraction.text import TfidfVectorizer
from sentence_transformers import SentenceTransformer, util
from Main import Main

# X·ª≠ l√Ω b·ªô nh·ªõ
gc.collect()
torch.cuda.empty_cache()
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

# Load sentence embedding model
comp_model = SentenceTransformer('all-MiniLM-L6-V2')

# C·∫•u h√¨nh giao di·ªán
st.set_page_config(page_title='Flan/Mistral Prompt Playground', layout='wide')

# Mapping task
task_map = {
    "classification": "classification",
    "simple qa": "qa_knowledge",
    "reasoning (social text)": "reasoning",
    "math": "computation"
}

# Sidebar
with st.sidebar:
    st.title("‚öôÔ∏è C·∫•u h√¨nh")
    selected_task = st.selectbox("Select task", list(task_map.keys()), key="task_selector")
    prompt_type = st.selectbox("Select prompt type", [
        "Direct zero-shot", "Zero-shot CoT",
        "Zero-shot CoT + Self-consistency", "Zero-shot ToT",
        "Zero-shot ToT expanded", "Direct few-shots",
        "Few-shots CoT", "Few-shots CoT + Self-consistency",
        "Few-shots ToT", "Few-shots ToT expanded",
        "Few-shots CoT + ART"
    ])
    show_prompt = st.checkbox("Hi·ªÉn th·ªã prompt ?", value=True)

st.title("ü§ñ Chat with Phi2")

# Load m√¥ h√¨nh n·∫øu ch∆∞a c√≥ ho·∫∑c task thay ƒë·ªïi
if (
    "model" not in st.session_state
    or "tokenizer" not in st.session_state
    or st.session_state.get("last_task") != selected_task
):
    with st.spinner("üîÑ ƒêang t·∫£i l·∫°i m√¥ h√¨nh..."):
        model_id = "microsoft/phi-2"
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            device_map=None
        ).eval().to("cuda")

        st.session_state.tokenizer = tokenizer
        st.session_state.model = model
        st.session_state.last_task = selected_task

# L∆∞u l·ªãch s·ª≠
if "history" not in st.session_state:
    st.session_state["history"] = []

# Input
user_input = st.text_area("Nh·∫≠p c√¢u h·ªèi ho·∫∑c n·ªôi dung:", height=100)

# X·ª≠ l√Ω khi nh·∫•n n√∫t
if st.button("Enter") and user_input.strip():
    tokenizer = st.session_state.tokenizer
    model = st.session_state.model
    name_task = task_map[selected_task]

    # N·∫øu l√† task math, load d·ªØ li·ªáu v√† t√≠nh TF-IDF
    tfidf_vectorizer = None
    X = None
    if name_task == "computation":
        with open('/content/final_train_math.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        task_samples = data[:350]
        new_tasks = []
        for task in task_samples:
            task["chain_of_thought"] = task['output']
            new_tasks.append(task)
        texts = [task['input'] for task in new_tasks]
        tfidf_vectorizer = TfidfVectorizer().fit(texts)
        X = tfidf_vectorizer.transform(texts)

    # Kh·ªüi t·∫°o handler
    task_handler = Main(
        tokenizer=tokenizer,
        model=model,
        comp_model=comp_model,
        name_task=name_task,
        X=X,
        vectorizer = vectorizer  # truy·ªÅn n·∫øu c√≥
    )

    with st.spinner("ƒêang x·ª≠ l√Ω..."):
        response = task_handler.main(
            user_input,
            name_prompt=prompt_type,
            do_print=show_prompt
        )

    st.session_state.history.append({"role": "user", "content": user_input})
    st.session_state.history.append({"role": "assistant", "content": response})

# Hi·ªÉn th·ªã tin nh·∫Øn g·∫ßn nh·∫•t
if st.session_state.history:
    for msg in st.session_state.history[-2:]:  # ch·ªâ hi·ªÉn th·ªã 1 v√≤ng t∆∞∆°ng t√°c
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])
